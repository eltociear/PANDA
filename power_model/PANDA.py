from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import BayesianRidge
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import BaggingRegressor
import xgboost as xgb
from sklearn.cluster import KMeans
from sklearn.metrics import r2_score
from sklearn.metrics import pairwise 
from sklearn.metrics import mean_absolute_percentage_error
import numpy as np
import math
import random
#import torch
#import torch.nn as nn
#import torch.nn.functional as f
import matplotlib.pyplot as plt

class PANDA_power_model:
    def __init__(self):
        self.microarch_dataset = np.load('../example_data/feature_set.npy')
        self.eda_dataset = np.load('../example_data/label_set.npy')
        self.microarch_dataset = self.microarch_dataset[:,1:,:]
        # For features,
        # 38-51 are the configuration parameters,
        # 52-205 are the events generated by gem5 (52-138 are events related to "Other Logic", 139-206 are events related to each component)
        self.flatten_microarch = self.microarch_dataset.reshape(self.microarch_dataset.shape[0]*self.microarch_dataset.shape[1],self.microarch_dataset.shape[2])
        self.flatten_eda = self.eda_dataset.reshape(self.eda_dataset.shape[0]*self.eda_dataset.shape[1],self.eda_dataset.shape[2])
        
        #what events are related to each component
        self.event_feature_of_components={
            "DCache":[139,149],
            "ICache":[149,155],
            "BP":[159,163],
            "RNU":[163,170],
            "Itlb":[170,172],
            "Dtlb":[172,174],
            "Regfile":[174,179],
            "ROB":[179,181],
            "IFU":[181,198],
            "LSU":[198,201],
            "FU_Pool":[201,203],
            "ISU":[203,206]
            
        }
        
        #what configuration parameters are related to each component
        self.params_feature_of_components={
            "DCache":[8,10,11,12],
            "ICache":[10,13],
            "BP":[0,7],
            "RNU":[1],
            "Itlb":[1],
            "Dtlb":[11],
            "Regfile":[1,4,5],
            "ROB":[1,3],
            "IFU":[0,1,2,13],
            "LSU":[6,8],
            "FU_Pool":[8,9],
            "ISU":[1,8,9]
        }
        
        #what configration parameters are taken into account for resource function
        self.encode_table={
            "BP":[0],
            "ICache":[0,1],
            "DCache":[0,1],
            "ISU":[0],
            "Logic":[1],
            "IFU":[1],
            "ROB":[1],
            "Regfile":[1,2],
            "RNU":[0],
            "Dtlb":[0],
            "LSU":[0]
            
        }
        self.component_model_selection=[0,0,0,0,0,0,1,1,0,1,0,1]
        self.others_model_selection=1
        self.logic_bias = 0
        self.dtlb_bias = 0
    
    
    def compute_reserve_station_entries(self,decodewidth_init):
        decodewidth = int(decodewidth_init+0.01)
        isu_params = [
            # IQT_MEM.numEntries IQT_MEM.dispatchWidth
            # IQT_INT.numEntries IQT_INT.dispatchWidth
            # IQT_FP.numEntries IQT_FP.dispatchWidth
            [8, decodewidth, 8, decodewidth, 8, decodewidth],
            [12, decodewidth, 20, decodewidth, 16, decodewidth],
            [16, decodewidth, 32, decodewidth, 24, decodewidth],
            [24, decodewidth, 40, decodewidth, 32, decodewidth],
            [24, decodewidth, 40, decodewidth, 32, decodewidth]
            ]
        _isu_params = isu_params[decodewidth - 1]
        return _isu_params[0]+_isu_params[2]+_isu_params[4]
    
    # train a ML model
    def train_sub_model(self,model,feature_set,label_set):
        model.fit(feature_set,label_set)
        return model
    
    
    def estimate_bias_logic(self,feature,label):
        feature_list = [int(feature[item]+0.01) for item in range(feature.shape[0])]
        num_of_feature = len(set(feature_list))
        if num_of_feature<=2:
            self.logic_bias = 4
        else:
            reg = LinearRegression().fit(feature.reshape(feature.shape[0],1), label.reshape(label.shape[0],1))
            bias = reg.intercept_
            alpha = reg.coef_[0]
            self.logic_bias = bias / alpha     
        return
    
    
    def estimate_bias_dtlb(self,feature,label):
        feature_list = [int(feature[item]+0.01) for item in range(feature.shape[0])]
        num_of_feature = len(set(feature_list))
        if num_of_feature<=1:
            self.dtlb_bias = 8
        else:
            reg = LinearRegression().fit(feature.reshape(feature.shape[0],1), label.reshape(label.shape[0],1))
            bias = reg.intercept_
            alpha = reg.coef_[0]
            self.dtlb_bias = bias / alpha       
        return
    
    
    # process resource function when training
    def encode_arch_knowledge(self,component_name,feature,label):
        if component_name=="BP" or component_name=="ICache" or component_name=="DCache" or component_name=="RNU" or component_name=="ROB" or component_name=="IFU" or component_name=="LSU":
            scale_factor = np.ones(label.shape)
            for i in range(len(self.encode_table[component_name])):
                encode_index = self.encode_table[component_name][i] + self.event_feature_of_components[component_name][1] - self.event_feature_of_components[component_name][0]
                acc_feature = feature[:,encode_index]
                scale_factor = scale_factor * acc_feature
            encode_label = label / scale_factor
        elif component_name=="Regfile":
            scale_factor = np.zeros(label.shape)
            for i in range(len(self.encode_table[component_name])):
                encode_index = self.encode_table[component_name][i] + self.event_feature_of_components[component_name][1] - self.event_feature_of_components[component_name][0]
                acc_feature = feature[:,encode_index]
                scale_factor = scale_factor + acc_feature
            encode_label = label / scale_factor
        elif component_name=="ISU":
            encode_index = self.encode_table[component_name][0] + self.event_feature_of_components[component_name][1] - self.event_feature_of_components[component_name][0]
            decodewidth = feature[:,encode_index]
            reserve_station = np.array([self.compute_reserve_station_entries(decodewidth[i]) for i in range(decodewidth.shape[0])])
            encode_label = label / reserve_station
        elif component_name=="Logic":
            encode_index = self.encode_table[component_name][0]
            self.estimate_bias_logic(feature[:,encode_index],label)
            encode_label = label / (feature[:,encode_index] + self.logic_bias)
        elif component_name=="Dtlb":
            encode_index = self.encode_table[component_name][0] + self.event_feature_of_components[component_name][1] - self.event_feature_of_components[component_name][0]
            self.estimate_bias_dtlb(feature[:,encode_index],label)
            encode_label = label / (feature[:,encode_index] + self.dtlb_bias)
        return encode_label
    
    
    # process resource function when predicting
    def decode_arch_knowledge(self,component_name,feature,pred):
        if component_name=="BP" or component_name=="ICache" or component_name=="DCache" or component_name=="RNU" or component_name=="ROB" or component_name=="IFU" or component_name=="LSU":
            scale_factor = np.ones(pred.shape)
            for i in range(len(self.encode_table[component_name])):
                decode_index = self.encode_table[component_name][i] + self.event_feature_of_components[component_name][1] - self.event_feature_of_components[component_name][0]
                acc_feature = feature[:,decode_index]
                scale_factor = scale_factor * acc_feature
            decode_pred = pred * scale_factor
        elif component_name=="Regfile":
            scale_factor = np.zeros(pred.shape)
            for i in range(len(self.encode_table[component_name])):
                decode_index = self.encode_table[component_name][i] + self.event_feature_of_components[component_name][1] - self.event_feature_of_components[component_name][0]
                acc_feature = feature[:,decode_index]
                scale_factor = scale_factor + acc_feature
            decode_pred = pred * scale_factor
        elif component_name=="ISU":
            decode_index = self.encode_table[component_name][0] + self.event_feature_of_components[component_name][1] - self.event_feature_of_components[component_name][0]
            decodewidth = feature[:,decode_index]
            reserve_station = np.array([self.compute_reserve_station_entries(decodewidth[i]) for i in range(decodewidth.shape[0])])
            decode_pred = pred * reserve_station
        elif component_name=="Logic":
            decode_index = self.encode_table[component_name][0]
            decode_pred = pred * (feature[:,decode_index] + self.logic_bias)
        elif component_name=="Dtlb":
            decode_index = self.encode_table[component_name][0] + self.event_feature_of_components[component_name][1] - self.event_feature_of_components[component_name][0]
            decode_pred = pred * (feature[:,decode_index] + self.dtlb_bias)
        return decode_pred
    
    
    # build ML model part for one component 
    def build_per_component_model(self, model, training_index, feature_index, label_index, component_name):
        component_feature = self.flatten_microarch[:,feature_index]
        component_feature = component_feature[training_index,:]
        component_label = self.flatten_eda[:,label_index]
        component_label = component_label[training_index]
        if component_name=="BP" or component_name=="ICache" or component_name=="DCache" or component_name=="ISU" or component_name=="RNU" or component_name=="ROB" or component_name=="IFU" or component_name=="Regfile" or component_name=="Dtlb" or component_name=="LSU":
            component_label = self.encode_arch_knowledge(component_name,component_feature,component_label)
        return_model = self.train_sub_model(model,component_feature,component_label)
        return return_model
    
    
    # predicting for one component
    def pred_per_component_result(self, model, testing_index, feature_index, component_name):
        component_feature = self.flatten_microarch[:,feature_index]
        component_feature = component_feature[testing_index,:]
        pred = model.predict(component_feature)
        if component_name=="BP" or component_name=="ICache" or component_name=="DCache" or component_name=="ISU" or component_name=="RNU" or component_name=="ROB" or component_name=="IFU" or component_name=="Regfile" or component_name=="Dtlb" or component_name=="LSU":
            pred = self.decode_arch_knowledge(component_name, component_feature, pred)
        return pred
    
    
    # build models for each component except "Other Logic"
    def build_all_component_level_model(self, training_index,model_selection_list):
        model_list = []
        iter = 0
        for keys in self.event_feature_of_components.keys():
            start = (self.event_feature_of_components[keys])[0]
            end = (self.event_feature_of_components[keys])[1]
            feature_index = [item for item in range(start,end)]
            param_index = self.params_feature_of_components[keys]
            param_index = [item+38 for item in param_index]
            feature_index = feature_index + param_index
            label_index = 6+len(self.event_feature_of_components)+iter
            model_selection = model_selection_list[iter]
            iter = iter + 1
            if model_selection==0:
                power_model = xgb.XGBRegressor()
            elif model_selection==1:
                power_model = GradientBoostingRegressor()
                
            return_model = self.build_per_component_model(power_model,training_index,feature_index,label_index,keys)
            model_list.append(return_model)
        return model_list
    
    
    # predicting for each component except "Other Logic"
    def pred_all_component_level_result(self, testing_index, model_list):
        iter = 0
        pred_list = []
        for keys in self.event_feature_of_components.keys():
            start = (self.event_feature_of_components[keys])[0]
            end = (self.event_feature_of_components[keys])[1]
            feature_index = [item for item in range(start,end)]
            param_index = self.params_feature_of_components[keys]
            param_index = [item+38 for item in param_index]
            feature_index = feature_index + param_index
            model = model_list[iter]
            pred = self.pred_per_component_result(model,testing_index,feature_index,keys)
            pred_list.append(pred)
            iter = iter + 1
        return pred_list
    
    
    # build model for "Other Logic"
    def build_others_model(self, training_index, model_selection):
        feature = self.flatten_microarch[training_index,:]
        feature = feature[:,38:139]
        label_total = self.flatten_eda[training_index,:]
        label = label_total[:,4]
        for i in range(len(self.params_feature_of_components)):
            component_index = 6+len(self.event_feature_of_components)+i
            label_tmp = label_total[:,component_index]
            label = label - label_tmp
        
        label = self.encode_arch_knowledge("Logic",feature,label)
        
        if model_selection==0:
            power_model = xgb.XGBRegressor()
        elif model_selection==1:
            power_model = GradientBoostingRegressor()
        power_model.fit(feature,label)
        return power_model
    
    
    # predicting total power for testing set
    def test_whole_model(self, testing_index, model_top, model_bottom):
        feature = self.flatten_microarch[testing_index,:]
        feature = feature[:,38:139]
        others_pred = model_top.predict(feature)
        others_pred = self.decode_arch_knowledge("Logic", feature, others_pred)
        component_pred = self.pred_all_component_level_result(testing_index,model_bottom)
        label_total = self.flatten_eda[testing_index,:]
        label = label_total[:,4]
        for i in range(len(self.params_feature_of_components)):
            component_index = 6+len(self.event_feature_of_components)+i
            label_tmp = label_total[:,component_index]
            label = label - label_tmp
        for i in range(len(self.params_feature_of_components)):
            component_index = 6+len(self.event_feature_of_components)+i
            label_tmp = label_total[:,component_index]
        pred = others_pred
        for i in range(len(component_pred)):
            pred = pred + component_pred[i]
        label = self.flatten_eda[testing_index,:]
        label = label[:,4]
        r2 = r2_score(label,pred)
        mape = mean_absolute_percentage_error(label,pred)
        return r2, mape, pred, label
    
    
    # train and then test when n configurations of 15 are unknown
    def unknown_n_config(self,unknown):
        fold = 15
        test_size = 8 * unknown
        pred_list = []
        label_list = []
        pred_acc_vector = np.zeros((15*8))
        
        for i in range(fold):
            start_point = 8*i
            end_point = start_point + test_size
            if end_point<=self.flatten_microarch.shape[0]:
                testing_set = [item for item in range(start_point,end_point)]
                training_set = [item for item in range(0,start_point)] + [item for item in range(end_point,self.flatten_microarch.shape[0])]
            else:
                end_point = end_point - self.flatten_microarch.shape[0]
                testing_set = [item for item in range(start_point,self.flatten_microarch.shape[0])] + [item for item in range(0,end_point)]
                training_set = [item for item in range(end_point,start_point)]
                
            model_others = self.build_others_model(training_set,self.others_model_selection)
            model_components = self.build_all_component_level_model(training_set,self.component_model_selection)
            r2, mape, pred, label = self.test_whole_model(testing_set, model_others, model_components)
            
            pred_list = pred_list + pred.tolist()
            label_list = label_list + label.tolist()
            pred_acc_vector[testing_set] = pred_acc_vector[testing_set] + pred
        
        
        r_report = np.corrcoef(label_list,pred_list)[1][0]
        mape_report = mean_absolute_percentage_error(label_list,pred_list)
        pred_acc_vector = pred_acc_vector / unknown
        label = self.flatten_eda[:,4]
        
        
        plt.clf()
        from matplotlib import rcParams
        rcParams.update({'figure.autolayout': True})
        plt.figure(figsize=(6, 5))
        plt.plot([0.4,1.6],[0.4,1.6],color='silver')
        color_set = ['b','g','r','c','m','y','k','skyblue','olive','gray','coral','gold','peru','pink','cyan','']
        for i in range(15):
            x = label[i*8:(i+1)*8]
            y = pred_acc_vector[i*8:(i+1)*8]
            plt.scatter(x,y,marker='.',color=color_set[i],label="BoomConfig{}".format(i),alpha=0.5,s=160)
        plt.xlabel('Ground Truth (W)', fontsize=22)
        plt.ylabel('Prediction (W)', fontsize=22)
        plt.xticks(fontsize=20)
        plt.yticks(fontsize=20)
        r_report = np.corrcoef(label,pred_acc_vector)[1][0]
        mape_report = mean_absolute_percentage_error(label,pred_acc_vector)
        print("Unknown_{}_config".format(unknown))
        print("R = {}".format(r_report))
        print("MAPE = {}%".format(mape_report * 100))  
        plt.text(0.4,1.4,"MAPE={:.2f}%\nR={:.2f}".format(mape_report*100,r_report),fontsize=20,bbox=dict(boxstyle='round,pad=0.5', fc='white', ec='silver',lw=5 ,alpha=0.7))
        ax = plt.gca()
        for axis in ['top','bottom','left','right']:
            ax.spines[axis].set_linewidth(2.5)
        plt.savefig("../result_figure/PANDA_power_model_unknown_{}.jpg".format(unknown),dpi=200)
        
        return r_report, mape_report
        
        
        
Calibration = PANDA_power_model()

curve_mape = []
curve_r = []
for i in range(1,15):
    r,mape = Calibration.unknown_n_config(i)
    curve_mape.append(mape)
    curve_r.append(r)
np.save("our_curve_mape.npy",np.array(curve_mape))
np.save("our_curve_r.npy",np.array(curve_r))

